{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4d2e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV comparison (higher R2, lower RMSE is better):\n",
      "        model  r2_mean   r2_std  rmse_mean  rmse_std\n",
      "         GBM 0.770568 0.025891   2.938230  0.048287\n",
      "      HistGB 0.767260 0.025729   2.960164  0.061489\n",
      "RandomForest 0.763304 0.026848   2.984586  0.055564\n",
      "       Ridge 0.761176 0.026209   2.998310  0.045655\n",
      "         SVR 0.755081 0.027871   3.035366  0.029482\n",
      "  ExtraTrees 0.752220 0.028473   3.053181  0.050328\n",
      "         KNN 0.745423 0.025791   3.097016  0.040273\n",
      "       Lasso 0.706489 0.018353   3.332034  0.089883\n",
      "  ElasticNet 0.701451 0.015181   3.362296  0.111417\n",
      "\n",
      "Selected best model: GBM\n",
      "{'test_model': 'GBM', 'test_RMSE': 3.059027476576049, 'test_R2': 0.7502706356546678}\n",
      "Anthony Davis -> 20.73\n",
      "DeMar DeRozan -> 24.45\n",
      "Zach LaVine -> 22.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "STATS_PATH = \"kaggle_seasons.csv\"            # stats file (renamed)\n",
    "ROSTER_PATH = \"player_teams_2025_26.csv\"     # 2025-26 roster file (Team + semicolon \"Roster\")\n",
    "\n",
    "# ------------ helpers\n",
    "\n",
    "def find_col(cols, candidates):\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in low:\n",
    "            return low[cand.lower()]\n",
    "    return None\n",
    "\n",
    "\n",
    "def season_to_year(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s)\n",
    "    if \"-\" in s:\n",
    "        a, b = s.split(\"-\", 1)\n",
    "        a = int(a)\n",
    "        b = int(b) if len(b) == 4 else int(str(a)[:2] + b)\n",
    "        return b\n",
    "    return int(float(s))\n",
    "\n",
    "\n",
    "def normalize_name(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    s = str(name).strip()\n",
    "    s = s.replace(\"â€™\", \"'\").replace(\".\", \"\")\n",
    "    s = s.replace(\" Jr\", \"\").replace(\" Jr.\", \"\")\n",
    "    s = s.replace(\" III\", \"\").replace(\" II\", \"\")\n",
    "    return s\n",
    "\n",
    "\n",
    "TEAM_MAP = {\n",
    "    'BRK': 'BKN', 'NJN': 'BKN',\n",
    "    'CHO': 'CHA', 'CHH': 'CHA', 'CHA': 'CHA',\n",
    "    'NOH': 'NOP', 'NOK': 'NOP', 'NOP': 'NOP',\n",
    "    'SEA': 'OKC', 'OKC': 'OKC',\n",
    "    'PHO': 'PHX',\n",
    "    'WSB': 'WAS',\n",
    "    'VAN': 'MEM', 'MEM': 'MEM',\n",
    "}\n",
    "\n",
    "# ------------ load & standardize stats\n",
    "raw = pd.read_csv(STATS_PATH)\n",
    "\n",
    "player_col = find_col(raw.columns, ['Player', 'player_name', 'Name'])\n",
    "team_col   = find_col(raw.columns, ['Tm', 'team_abbreviation', 'Team'])\n",
    "season_col = find_col(raw.columns, ['Year', 'season', 'Season'])\n",
    "pts_col    = find_col(raw.columns, ['PTS', 'pts', 'pts_per_game'])\n",
    "g_col      = find_col(raw.columns, ['G', 'gp', 'games', 'games_played'])\n",
    "mp_col     = find_col(raw.columns, ['MP', 'mp', 'min', 'minutes'])\n",
    "\n",
    "if not all([player_col, team_col, season_col, pts_col]):\n",
    "    raise ValueError(\"Required columns not found. Need player, team, season, points.\")\n",
    "\n",
    "stats = raw.copy()\n",
    "stats['Player'] = stats[player_col].map(normalize_name)\n",
    "stats['Tm']     = stats[team_col]\n",
    "stats['Year']   = stats[season_col].apply(season_to_year)\n",
    "stats['PTS']    = pd.to_numeric(stats[pts_col], errors='coerce')\n",
    "stats['G']      = pd.to_numeric(stats[g_col], errors='coerce') if g_col else np.nan\n",
    "stats['MP']     = pd.to_numeric(stats[mp_col], errors='coerce') if mp_col else np.nan\n",
    "stats['Tm_norm']= stats['Tm'].map(TEAM_MAP).fillna(stats['Tm'])\n",
    "\n",
    "# choose one row per (Player, Year): highest games\n",
    "stats['_g_rank'] = stats.groupby(['Player', 'Year'])['G'].rank(ascending=False, method='first')\n",
    "base = (\n",
    "    stats.sort_values(['Player', 'Year', '_g_rank'])\n",
    "         .drop_duplicates(['Player', 'Year'])\n",
    "         .drop(columns=['_g_rank'])\n",
    ")\n",
    "\n",
    "# targets and next team from stats itself\n",
    "base = base.sort_values(['Player', 'Year'])\n",
    "base['PTS_next'] = base.groupby('Player')['PTS'].shift(-1)\n",
    "\n",
    "next_primary = (\n",
    "    stats.assign(Year=stats['Year'])\n",
    "         .sort_values(['Player', 'Year', 'G'], ascending=[True, True, False])\n",
    "         .drop_duplicates(['Player', 'Year'])[['Player', 'Year', 'Tm_norm']]\n",
    "         .rename(columns={'Tm_norm': 'team_next'})\n",
    ")\n",
    "next_primary['Year'] = next_primary['Year'] - 1\n",
    "base = base.merge(next_primary, on=['Player', 'Year'], how='left')\n",
    "\n",
    "# keep rows with a real next season & known team\n",
    "base = base[(base['PTS_next'].notna()) & (base['team_next'].notna())]\n",
    "\n",
    "# last 20 seasons\n",
    "max_year = int(base['Year'].max())\n",
    "cutoff   = max_year - 19\n",
    "train_df = base[base['Year'] >= cutoff].copy()\n",
    "\n",
    "# drop junk index cols if present\n",
    "junk = [c for c in train_df.columns if c.strip().lower() in (\"unnamed: 0\", \"unnamed:0\", \"index\")]\n",
    "if junk:\n",
    "    train_df.drop(columns=junk, inplace=True)\n",
    "\n",
    "# teammate context (leave-one-out means) by (Year, team_next)\n",
    "# only include columns that exist AND have at least one non-null value\n",
    "cand_cols = [c for c in ['PTS', 'MP'] if c in train_df.columns and train_df[c].notna().any()]\n",
    "grp = train_df.groupby(['Year', 'team_next'])\n",
    "team_sum = grp[cand_cols].sum().add_prefix('sum_') if cand_cols else pd.DataFrame()\n",
    "team_cnt = grp.size().rename('cnt_players')\n",
    "team_aggs = pd.concat([team_sum, team_cnt], axis=1).reset_index()\n",
    "train_df = train_df.merge(team_aggs, on=['Year', 'team_next'], how='left')\n",
    "\n",
    "for col in cand_cols:\n",
    "    train_df[f'teammates_mean_{col}'] = (\n",
    "        (train_df[f'sum_{col}'] - train_df[col]) / (train_df['cnt_players'] - 1).clip(lower=1)\n",
    "    )\n",
    "train_df.drop(columns=[c for c in train_df.columns if c.startswith('sum_')] + ['cnt_players'], inplace=True, errors='ignore')\n",
    "\n",
    "# --------- feature lists (avoid duplicates!)\n",
    "drop_cols = ['Player', 'Year', 'Tm', 'Tm_norm', 'team_next', 'PTS_next']\n",
    "# numeric base\n",
    "numeric_base = train_df.select_dtypes('number').columns.tolist()\n",
    "# teammate features\n",
    "teammate_feats = [c for c in train_df.columns if c.startswith('teammates_mean_')]\n",
    "# numeric feats exclude target/ids & keep unique\n",
    "numeric_feats = [c for c in numeric_base if c not in drop_cols and not c.startswith('teammates_mean_')]\n",
    "# remove columns that are entirely NaN\n",
    "numeric_feats = [c for c in numeric_feats if not train_df[c].isna().all()]\n",
    "# combine with teammate feats\n",
    "num_cols = list(dict.fromkeys(numeric_feats + teammate_feats))\n",
    "cat_feats = ['team_next']\n",
    "all_feature_cols = list(dict.fromkeys(num_cols + cat_feats))\n",
    "\n",
    "# OHE compatibility across sklearn versions\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# small factory so we can optionally scale for non-tree models\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def make_preprocessor(scale_numeric: bool):\n",
    "    steps = [('imputer', SimpleImputer(strategy='median'))]\n",
    "    if scale_numeric:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    num_pipe = Pipeline(steps)\n",
    "    return ColumnTransformer([\n",
    "        ('num', num_pipe, num_cols),\n",
    "        ('cat', ohe, cat_feats)\n",
    "    ])\n",
    "\n",
    "# candidate models: (estimator, needs_scaling)\n",
    "models = {\n",
    "    'RandomForest': (RandomForestRegressor(n_estimators=700, random_state=42, n_jobs=-1), False),\n",
    "    'ExtraTrees'  : (ExtraTreesRegressor(n_estimators=700, random_state=42, n_jobs=-1), False),\n",
    "    'HistGB'      : (HistGradientBoostingRegressor(random_state=42), False),\n",
    "    'GBM'         : (GradientBoostingRegressor(random_state=42), False),\n",
    "    'Ridge'       : (Ridge(), True),\n",
    "    'Lasso'       : (Lasso(max_iter=5000), True),\n",
    "    'ElasticNet'  : (ElasticNet(max_iter=5000), True),\n",
    "    'SVR'         : (SVR(), True),\n",
    "    'KNN'         : (KNeighborsRegressor(n_neighbors=15), True),\n",
    "}\n",
    "\n",
    "# --- build X, y\n",
    "X = train_df[all_feature_cols]\n",
    "y = train_df['PTS_next']\n",
    "\n",
    "# --- compare models with 5-fold CV (R2 and RMSE)\n",
    "results = []\n",
    "for name, (est, need_scale) in models.items():\n",
    "    pre = make_preprocessor(scale_numeric=need_scale)\n",
    "    pipe = Pipeline([('prep', pre), ('model', est)])\n",
    "    scores = cross_validate(\n",
    "        pipe, X, y,\n",
    "        cv=5,\n",
    "        scoring={'r2': 'r2', 'rmse': 'neg_root_mean_squared_error'},\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    r2_mean = scores['test_r2'].mean()\n",
    "    r2_std  = scores['test_r2'].std()\n",
    "    rmse_mean = -scores['test_rmse'].mean()\n",
    "    rmse_std  =  scores['test_rmse'].std()\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'r2_mean': r2_mean,\n",
    "        'r2_std': r2_std,\n",
    "        'rmse_mean': rmse_mean,\n",
    "        'rmse_std': rmse_std\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(results).sort_values(['r2_mean', 'rmse_mean'], ascending=[False, True])\n",
    "print(\"\\nCV comparison (higher R2, lower RMSE is better):\\n\", cv_df.to_string(index=False))\n",
    "\n",
    "# --- choose the best by R2 then RMSE\n",
    "best_name = cv_df.iloc[0]['model']\n",
    "best_est, best_scale = models[best_name]\n",
    "print(f\"\\nSelected best model: {best_name}\")\n",
    "\n",
    "# --- final train/test evaluation with the winner\n",
    "pre_best = make_preprocessor(scale_numeric=best_scale)\n",
    "best_pipe = Pipeline([('prep', pre_best), ('model', best_est)])\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "best_pipe.fit(X_tr, y_tr)\n",
    "\n",
    "y_hat = best_pipe.predict(X_te)\n",
    "rmse = float(np.sqrt(mean_squared_error(y_te, y_hat)))\n",
    "r2   = float(r2_score(y_te, y_hat))\n",
    "print({\n",
    "    'test_model': best_name,\n",
    "    'test_RMSE': rmse,\n",
    "    'test_R2': r2\n",
    "})\n",
    "\n",
    "# ------------ inference for 2025-26 using roster (still works with the chosen model)\n",
    "ros_raw = pd.read_csv(ROSTER_PATH)\n",
    "r_team = find_col(ros_raw.columns, ['Team'])\n",
    "r_rost = find_col(ros_raw.columns, ['Roster'])\n",
    "if not all([r_team, r_rost]):\n",
    "    raise ValueError(\"Roster file must have columns for Team and Roster (semicolon-separated).\")\n",
    "\n",
    "ros = ros_raw[[r_team, r_rost]].copy()\n",
    "ros['Player'] = ros[r_rost].astype(str).str.split(';')\n",
    "ros = ros.explode('Player')\n",
    "ros['Player'] = ros['Player'].astype(str).str.strip().map(normalize_name)\n",
    "ros = ros.rename(columns={r_team: 'team_2025_26'})[['Player', 'team_2025_26']].drop_duplicates()\n",
    "\n",
    "# base for inference: last season rows from the already-built base table\n",
    "base_inf = base[base['Year'] == max_year].copy()\n",
    "inf_df = base_inf.merge(ros, on='Player', how='left')\n",
    "inf_df['team_2025_26'] = inf_df['team_2025_26'].fillna('Unknown')\n",
    "\n",
    "# compute teammate context for 2025-26 using last-season stats grouped by 2025-26 team\n",
    "cand_cols_inf = [c for c in ['PTS', 'MP'] if c in inf_df.columns and inf_df[c].notna().any()]\n",
    "grp_inf = inf_df.groupby('team_2025_26')\n",
    "team_sum_inf = grp_inf[cand_cols_inf].sum().add_prefix('sum_') if cand_cols_inf else pd.DataFrame()\n",
    "team_cnt_inf = grp_inf.size().rename('cnt_players')\n",
    "team_aggs_inf = pd.concat([team_sum_inf, team_cnt_inf], axis=1).reset_index()\n",
    "inf_df = inf_df.merge(team_aggs_inf, on='team_2025_26', how='left')\n",
    "for col in cand_cols_inf:\n",
    "    inf_df[f'teammates_mean_{col}'] = (\n",
    "        (inf_df[f'sum_{col}'] - inf_df[col]) / (inf_df['cnt_players'] - 1).clip(lower=1)\n",
    "    )\n",
    "inf_df.drop(columns=[c for c in inf_df.columns if c.startswith('sum_')] + ['cnt_players'], inplace=True, errors='ignore')\n",
    "inf_df['team_next'] = inf_df['team_2025_26']  # align with training categorical\n",
    "\n",
    "# columns for inference must match training\n",
    "feature_cols_inf = list(dict.fromkeys(num_cols + cat_feats))\n",
    "\n",
    "\n",
    "def predict_ppg_2025(player_name: str) -> float:\n",
    "    row = inf_df[inf_df['Player'] == normalize_name(player_name)]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"{player_name} not found in last-season base or 2025-26 roster.\")\n",
    "    return float(best_pipe.predict(row[feature_cols_inf])[0])\n",
    "\n",
    "# quick spot-check\n",
    "for p in [\"Anthony Davis\", \"DeMar DeRozan\", \"Zach LaVine\"]:\n",
    "    try:\n",
    "        print(p, \"->\", round(predict_ppg_2025(p), 2))\n",
    "    except Exception as e:\n",
    "        print(p, \"->\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473d8d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best CV R2: 0.7811520474902518\n",
      "Best params: {'model__subsample': 0.7, 'model__n_estimators': 1600, 'model__min_samples_split': 10, 'model__min_samples_leaf': 15, 'model__max_features': 0.5, 'model__max_depth': 3, 'model__loss': 'huber', 'model__learning_rate': 0.015, 'model__alpha': 0.95}\n",
      "{'GBM_test_RMSE': 3.036927728602918, 'GBM_test_R2': 0.7538659092398875}\n",
      "Anthony Davis -> 20.34\n",
      "DeMar DeRozan -> 24.47\n",
      "Zach LaVine -> 22.55\n",
      "LeBron James -> 26.14\n",
      "Luka Doncic -> 28.57\n",
      "Jimmy Butler -> 20.13\n"
     ]
    }
   ],
   "source": [
    "# Assumes you've already built:\n",
    "# - train_df, num_cols, cat_feats\n",
    "# - inf_df, feature_cols_inf, normalize_name\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# OneHotEncoder version-safe\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "    ('cat', ohe, cat_feats),\n",
    "])\n",
    "\n",
    "X = train_df[num_cols + cat_feats]\n",
    "y = train_df['PTS_next']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "gbm = GradientBoostingRegressor(\n",
    "    random_state=42,\n",
    "    validation_fraction=0.1,   # early stopping\n",
    "    n_iter_no_change=15,\n",
    "    tol=1e-4,\n",
    "    loss='squared_error'       # we'll also try 'huber' via the search space below\n",
    ")\n",
    "\n",
    "pipe = Pipeline([('prep', pre), ('model', gbm)])\n",
    "\n",
    "# Local search around your best params\n",
    "param_dist = {\n",
    "    \"model__learning_rate\":   [0.015, 0.02, 0.025, 0.03],\n",
    "    \"model__n_estimators\":    [1200, 1600, 2000, 2400],\n",
    "    \"model__max_depth\":       [2, 3, 4],\n",
    "    \"model__min_samples_leaf\":[5, 10, 15, 20],\n",
    "    \"model__min_samples_split\":[5, 10, 20, 40],\n",
    "    \"model__subsample\":       [0.6, 0.7, 0.8, 0.9],\n",
    "    \"model__max_features\":    [\"sqrt\", 0.3, 0.5, 0.7, None],\n",
    "    \"model__loss\":            [\"squared_error\", \"huber\"],\n",
    "    \"model__alpha\":           [0.85, 0.9, 0.95],  # only used if loss='huber'\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,             # you said 150 fits took ~30s; this should be fine\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "search.fit(X_tr, y_tr)\n",
    "best = search.best_estimator_\n",
    "print(\"Best CV R2:\", search.best_score_)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "\n",
    "# Optionally expand the forest if the LR is small (post-tune bump)\n",
    "lr = best.get_params()[\"model__learning_rate\"]\n",
    "trees = best.get_params()[\"model__n_estimators\"]\n",
    "if lr <= 0.02 and trees < 3000:\n",
    "    best.set_params(model__n_estimators=3000)\n",
    "\n",
    "# Final fit on the training split\n",
    "best.fit(X_tr, y_tr)\n",
    "\n",
    "# Held-out test metrics\n",
    "pred = best.predict(X_te)\n",
    "rmse = float(np.sqrt(mean_squared_error(y_te, pred)))\n",
    "r2   = float(r2_score(y_te, pred))\n",
    "print({\"GBM_test_RMSE\": rmse, \"GBM_test_R2\": r2})\n",
    "\n",
    "# Use tuned GBM for 2025â€“26 predictions\n",
    "def predict_ppg_2025(name: str) -> float:\n",
    "    row = inf_df[inf_df['Player'] == normalize_name(name)]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"{name} not found in last-season base or 2025-26 roster.\")\n",
    "    return float(best.predict(row[feature_cols_inf])[0])\n",
    "\n",
    "for p in [\"Anthony Davis\",\"DeMar DeRozan\",\"Zach LaVine\", \"LeBron James\", \"Luka Doncic\", \"Jimmy Butler\"]:\n",
    "    try:\n",
    "        print(p, \"->\", round(predict_ppg_2025(p), 2))\n",
    "    except Exception as e:\n",
    "        print(p, \"->\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
